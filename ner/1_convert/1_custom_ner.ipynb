{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e9486b5-702b-4cae-b734-d00fae03d8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    }
   ],
   "source": [
    "# Generate a config from https://spacy.io/usage/training#quickstart\n",
    "# Using [ner]\n",
    "#\n",
    "# Followed by the command\n",
    "# python3 -m spacy init fill-config base_config.cfg config.cfg\n",
    "#\n",
    "# Then start training with\n",
    "# python3 -m spacy train config.cfg --output ./ --paths.train ./train.spacy --paths.dev ./train.spacy\n",
    "\n",
    "import spacy\n",
    "import json\n",
    "\n",
    "from spacy.tokens import DocBin      # for annotated data\n",
    "from spacy.util import filter_spans  # for removing spans with overlaps\n",
    "\n",
    "\n",
    "file = open(\"Corona2.json\", \"r\")\n",
    "\n",
    "# read raw file contents\n",
    "# print(file.read())\n",
    "\n",
    "data = json.load(file)\n",
    "\n",
    "# pretty print parsed JSON\n",
    "# print(json.dumps(data, indent=4))\n",
    "\n",
    "# pretty print single entry\n",
    "#print(json.dumps(data[\"examples\"][0], indent=4))\n",
    "\n",
    "training_data = {'classes' : ['MEDICINE', \"MEDICALCONDITION\", \"PATHOGEN\"], 'annotations' : []}\n",
    "\n",
    "# We only need the text string, the entity's (start index, end index, type)\n",
    "for example in data[\"examples\"]:\n",
    "    entry = {}\n",
    "    entry[\"text\"] = example[\"content\"]\n",
    "    entry[\"entities\"] = []\n",
    "    \n",
    "    for annotation in example[\"annotations\"]:\n",
    "        start = annotation['start']\n",
    "        end = annotation['end']\n",
    "        label = annotation['tag_name'].upper()\n",
    "        entry['entities'].append((start, end, label))\n",
    "  \n",
    "    training_data['annotations'].append(entry)\n",
    "\n",
    "\n",
    "# print converted data\n",
    "# print(json.dumps(training_data['annotations'][0], indent=4))\n",
    "\n",
    "\n",
    "# Convert the data to SpaCy's DocBin\n",
    "nlp = spacy.blank(\"de\")\n",
    "\n",
    "doc_bin = DocBin()\n",
    "for data in training_data[\"annotations\"]:\n",
    "    text = data['text']\n",
    "    labels = data['entities']\n",
    "    \n",
    "    doc = nlp.make_doc(text) \n",
    "    \n",
    "    ents = []\n",
    "    for start, end, label in labels:\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\") \n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    \n",
    "    filtered_ents = filter_spans(ents) # remove duplicates or overlaps\n",
    "    doc.ents = filtered_ents \n",
    "    doc_bin.add(doc)\n",
    "    \n",
    "# save the docbin object\n",
    "doc_bin.to_disk(\"train.spacy\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f456c6-6abf-4fda-afa0-a58ecc8104b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
